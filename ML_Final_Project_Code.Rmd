---
title: "ML Final Project"
author: "Atsumi Kainosho & Haley Townsend"
date: "May 8, 2019"
output: html_document
---

```{r}
# First, load libraries necessary for the coding part of the project.

library(Hmisc)
library(tidyr)
library(plyr)
library(dplyr)
library(readr)
library(haven)
library(foreign)
library(RNHANES)
library(purrr)
library(magrittr)
library(glmnet)
library(keras)
library(stringr)
library(data.table)
library(lubridate)
library(caret)
library(ROCR)
library(rpart)
library(rpart.plot)
library(glmnet)
library(randomForest)
library(ada)
library(gbm)
```

```{r}
# Import data set from National Health and Nutrition Examination Survey (NHANES) web site.
# We used National  Youth  Fitness  Survey  (NNYFS) for 2012, specifically, Demographic Variables & Sample Weights (Y_DEMO) and Body Measures (Y_BMX) datasets.
# Since the data format was SAS, we extracted data into CSV file first, and then read the file to use them in R.

# NHANES NNYFS  Demographic Variables & Sample Weights (Y_DEMO) data
# Please change the directry to yours before running
demo = read_xpt("C:\\Users\\atsum\\Documents\\GitHub\\ML-Final-Project\\Y_DEMO.xpt")
write.csv(demo, file = 'attempt.csv')
demodf = read_csv("attempt.csv")

# Body Measures (Y_BMX) data that includes BMI information
body = read_xpt('C:\\Users\\atsum\\Documents\\GitHub\\ML-Final-Project\\Y_BMX.xpt')
write.csv(body, file = 'body.csv')
bodydf=read_csv('body.csv')

```


```{r}
# After reading datasets, we implemented data cleaning. 

# First, we remove objects created to import SAS files in order to avoid confusions.
rm(demo)
rm(body)

# Remove the X1 columns from both dfs that manage row numbers
demodf = demodf %>% select(-X1)
bodydf = bodydf %>% select(-X1)

# Join the demographic data to the body data using SEQN that represents Respondent sequence number
df = left_join(bodydf, demodf, by = 'SEQN') 

# Look at where there is missing data 
colSums(is.na(df))

# Include only the demographic columns of interest and the Y variable (BMDBMIC)
df = df %>% select(RIAGENDR, RIDEXAGY,RIDRETH1, DMDBORN4, INDHHIN2, DMDHHSZA, BMDBMIC, SEQN) 

# Rename the columns, so they are easier to understand
names(df)[1]<-"gender"
names(df)[2]<-"age"
names(df)[3]<-"race"
names(df)[4]<-"birth_country"
names(df)[5]<-"annual_household_income"
names(df)[6] <-"num_children_5yrs_younger"
names(df)[7] <-"Y_BMI_category"

# Drop rows containing NA for the outcome (BMI) immediately
df = df %>% filter(!is.na(Y_BMI_category)) 

# Clean the outcome variable so it is a biclassification problem instead of multi-class
# Relevel the outcome or BMI (Y) to have just two levels: Y is obese (obese:4) and
#N is not obese (which includes underweight:1, normal:2, overweight:3)
df = df %>% mutate(Y = case_when(
  Y_BMI_category == 2 ~ 'N',
  Y_BMI_category == 1 ~ 'N',
  Y_BMI_category == 3 ~ 'N',
  Y_BMI_category == 4 ~ 'Y'
)) %>% select(-Y_BMI_category)

# Change levels of factors for predictors: gender, race, birth_country, and the outcome
cols <- c('gender', 'race', 'birth_country', 'Y')

df[cols] <- lapply(df[cols], factor)

# Gender, 1 is male and 2 is female
df$gender <- ordered(df$gender,
                     levels = c(1,2),
                     labels = c("male", "female"))

# Race, 1 is Mexican American, 2 is Other Hispanic, 3 Non-Hispanic White, 4 Non-Hispanic Black, 5 Other Race
df$race <- ordered(df$race,
                   levels = c(1,2,3,4,5),
                   labels = c("Mexican American", "Other Hispanic","White",
                              "Black", "Other"))

# birth_country, 1 is US, 2 is Other
df$birth_country <- ordered(df$birth_country,
                            levels = c(1,2),
                            labels = c("USA", "Other"))

# Modify the annual household income variable to enable us to use numeric income information instead of category.
# 77 represents "Refused" and 99 respresents "Don't Know", which equals to no income value information.
#We convert them as NA first, which gives us 45 new NAs.
df$annual_household_income = na_if(df$annual_household_income, 99)
df$annual_household_income = na_if(df$annual_household_income, 77)
sum(is.na(df$annual_household_income))

# Modify annual_household_income into numeric using the middle value in each group
df = df %>% mutate(annual_household_income_num = case_when(
  annual_household_income == 1 ~ 2500,
  annual_household_income == 2 ~ 7500,
  annual_household_income == 3 ~ 12500,
  annual_household_income == 4 ~ 17500,
  annual_household_income == 5 ~ 22500,
  annual_household_income == 6 ~ 30000,
  annual_household_income == 7 ~ 40000,
  annual_household_income == 8 ~ 50000,
  annual_household_income == 9 ~ 60000,
  annual_household_income == 10 ~ 70000,
  annual_household_income == 12 ~ 30000,
  annual_household_income == 13 ~ 10000,
  annual_household_income == 14 ~ 87500,
  annual_household_income == 15 ~ 100000
  )) %>% select(-annual_household_income)

# Remove unique ID from main dataframe
df = df %>% select(-SEQN)

# Drop rows incluging NAs
df = df %>% na.omit()

# Show summary of created dataset, which equals to Table 1 in the report.
df %>% summary()

```

```{r}
## Building a Base Model: Logistic Regression using base features: gender, age, race, and birth country

# Create a new dataset df_1 ontaining base features and the outcome
df_1 = df %>% select(gender, age, race, birth_country, Y)

# Set seed first to create training and test datasets
set.seed(518) # our graduation date!

# Shuffle sampes for randamization
shuffled_df_1 = df_1[sample(1:nrow(df_1)),]

# Assign 75% of the total samples into training set and 25% into test set.
# Traning set contains 1144 samples and test set has 382 samples.
n = round((0.75 * nrow(shuffled_df_1)),0)
train_1 = df_1[1:n,]
test_1 = df_1[-(1:n),]

# Confirm that train and test are balanced with the outcome of interest (Y)
train_1 %>% select(Y) %>% table() %>% prop.table()
test_1 %>% select(Y) %>% table() %>% prop.table()

# Run a base model, Logistic Regression
lr_1 = with(train_1, glm(Y=="Y" ~.,
                     family = binomial("logit"),
                     data = train_1))
lr_1 %>% summary()

```

```{r}
## Building Additional Models and Comparing Performance

# Under the estimation that ensemble methods can improve the model performance 
# since they aggregate multiple weak learners produced by small number of samples,
# we implemented Random Forest, AdaBoosting, and Gradient Boosted Forest.

# Implement Random Forest 
forest_1 = randomForest(formula = Y ~ .,
                      data=train_1, ntrees=10)

# Implement AdaBoosting
aforest_1 = ada(formula = Y=="Y" ~ .,
              data=train_1,
              iter=10)

# Implement Gradient Boosted Forest
gforest_1 = gbm(formula = Y=="Y" ~ .,
              data=train_1 %>%
                mutate_if(is.logical, as.factor),
              interaction.depth=10,
              cv.folds = 2)

# Since the sample size is small, to avoid overfitting, we see whether there are variables that are resularized with Lasso.

# First, we numerically normalize datasets to do Lasso-Regularized Logistic Regression
m_1 = model.matrix(Y~., df_1) 
mtrain_1 = m_1[1:n,]
mtest_1 = m_1[-(1:n),]

# Perform cross validation to find the best lambda
lasso_cv_1 = cv.glmnet(x= mtrain_1, y=train_1$Y, family="binomial")

# Store the minimum lambda as bestlalm_1
bestlam_1 = lasso_cv_1$lambda.min 

# Build a Lasso-Regularized Logistic Regression model 
lasso_1 = glmnet(x = mtrain_1, y=train_1$Y, alpha = 1, family="binomial")

# Look at coefficients with the best lambda
lasso_1.coef = predict(lasso_1, type="coefficients", s= bestlam_1)
lasso_1.coef

## To see performance of the new model, we calculated AUC and draw ROC curves
# Calculate AUC and prepare for ROC curve for Logistic Regression
logit_prediction_1 = predict(lr_1,test_1)
logit_rocdata_1 = prediction(predictions=logit_prediction_1,
                           labels=test_1$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_lg_1 = prediction(predictions=logit_prediction_1,
                     labels=test_1$Y) %>% performance("auc")
auc_lg_1 = round(auc_lg_1@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Random Forest
test_1$Y <- as.factor(test_1$Y)
rf_prediction_1 = predict(forest_1, test_1, type = "prob")
rf_rocdata_1 = prediction(predictions=rf_prediction_1[,2],
                        labels=test_1$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_rf_1 = prediction(predictions=rf_prediction_1[,2],
                      labels=test_1$Y) %>% performance("auc")
auc_rf_1 = round(auc_rf_1@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Boosting
boosting_prediction_1 = predict(aforest_1, test_1, type = "prob")
boosting_rocdata_1 = prediction(predictions=boosting_prediction_1[,2],
                              labels=test_1$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_boosting_1 = prediction(predictions=boosting_prediction_1[,2],
                          labels=test_1$Y) %>% performance("auc")
auc_boosting_1 = round(auc_boosting_1@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Gradient Boosted Forest
gradient_prediction_1 = predict(gforest_1, test_1,
                              n.trees=gforest_1$n.trees,type = "response")
gradient_rocdata_1 = prediction(predictions=gradient_prediction_1,
                              labels=test_1$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_gradient_1 = prediction(predictions=gradient_prediction_1,
                          labels=test_1$Y) %>% performance("auc")
auc_gradient_1 = round(auc_gradient_1@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Lasso-Regularized Logistic Regression
lasso_prediction_1 = predict(lasso_1, mtest_1, s= bestlam_1, type='response')
lasso_rocdata_1 = prediction(predictions=lasso_prediction_1,
                           labels=test_1$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_lasso_1 =  prediction(predictions=lasso_prediction_1,
                        labels=test_1$Y) %>% performance("auc")
auc_lasso_1 = round(auc_lasso_1@y.values[[1]],3)

# Plot ROC curve
ggplot(data = logit_rocdata_1, aes(x=FPR,y=TPR, col = paste0("Logistic Regression: AUC", auc_lg_1))) + geom_line() +
  geom_line(data = rf_rocdata_1, aes(x=FPR,y=TPR, col = paste0("RF: AUC", auc_rf_1))) +
  geom_line(data = boosting_rocdata_1, aes(x=FPR,y=TPR, col = paste0("Boosting: AUC", auc_boosting_1))) +
  geom_line(data = gradient_rocdata_1, aes(x=FPR,y=TPR, col = paste0("Gradient boosting: AUC", auc_gradient_1))) + 
  geom_line(data = lasso_rocdata_1, aes(x=FPR,y=TPR, col = paste0("Lasso: AUC", auc_lasso_1))) +
  theme(legend.title=element_blank())
```

```{r}
## Since we could not see the improvement of performance with ensemble methods, we add features to improve the model.
# Annual household income and nunmber of children younger than 5 years old are added.

# Create a new dataset 
df_2 = df %>% select(gender, age, race, birth_country, num_children_5yrs_younger, annual_household_income_num, Y)

# Set seed first to create training and test datasets
set.seed(518) # our graduation date!

# Shuffle sampes for randamization
shuffled_df_2 = df_2[sample(1:nrow(df_2)),]

# Assign 75% of the total samples into training set and 25% into test set.
# Traning set contains 1144 samples and test set has 382 samples.
n = round((0.75 * nrow(shuffled_df_2)),0)
train_2 = df_2[1:n,]
test_2 = df_2[-(1:n),]

# Confirm that train and test are balanced with the outcome of interest (Y)
train_2 %>% select(Y) %>% table() %>% prop.table()
test_2 %>% select(Y) %>% table() %>% prop.table()

# Run a base model, Logistic Regression, with new features 
lr_2 = with(train_2, glm(Y=="Y" ~ .,
                         family = binomial("logit"),
                         data = train_2))
lr_2 %>% summary()

# Then, we also run ensemble metehods with new features
# Run Random Forest
forest_2 = randomForest(formula = Y ~ .,
                        data=train_2, ntrees=10)
# Run Boosting
aforest_2 = ada(formula = Y=="Y" ~ .,
                data=train_2,
                iter=10)

# Run Gradient Boosted Forest
gforest_2 = gbm(formula = Y=="Y" ~ .,
                data=train_2 %>%
                  mutate_if(is.logical, as.factor),
                interaction.depth=10,
                cv.folds = 2)

# Since we added new features, we also want to see the performance of Lasso-Regularized Logistic Regression

# Numerically normalize datasets to do Lasso-Regularized Logistic Regression
m_2 = model.matrix(Y~., df_2) 
mtrain_2 = m_2[1:n,]
mtest_2 = m_2[-(1:n),]

# Perform cross validation to find the best lambda
lasso_cv_2 = cv.glmnet(x= mtrain_2, y=train_2$Y, family="binomial") 

# Store the minimum lambda as bestlalm_2
bestlam_2 = lasso_cv_2$lambda.min 

# Build a Lasso-Regularized Logistic Regression model
lasso_2 = glmnet(x = mtrain_2, y=train_2$Y, alpha = 1, family="binomial")

# Look at coefficients with the best lambda
lasso_2.coef = predict(lasso_2, type="coefficients", s= bestlam_2)
lasso_2.coef


## To see performance of the new model, we calculated AUC and draw ROC curves
# Calculate AUC and prepare for ROC curve for Logistic Regression
logit_prediction_2 = predict(lr_2,test_2)
logit_rocdata_2 = prediction(predictions=logit_prediction_2,
                             labels=test_2$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_lg_2 =  prediction(predictions=logit_prediction_2,
                       labels=test_2$Y) %>% performance("auc")
auc_lg_2 = round(auc_lg_2@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Random Forest
test_2$Y <- as.factor(test_2$Y)
rf_prediction_2 = predict(forest_2, test_2, type = "prob")
rf_rocdata_2 = prediction(predictions=rf_prediction_2[,2],
                          labels=test_2$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_rf_2 = prediction(predictions=rf_prediction_2[,2],
                      labels=test_2$Y) %>% performance("auc")
auc_rf_2 = round(auc_rf_2@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for Boosting
boosting_prediction_2 = predict(aforest_2, test_2, type = "prob")
boosting_rocdata_2 = prediction(predictions=boosting_prediction_2[,2],
                                labels=test_2$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_boosting_2 = prediction(predictions=boosting_prediction_2[,2],
                            labels=test_2$Y) %>% performance("auc")
auc_boosting_2 = round(auc_boosting_2@y.values[[1]],3)

# Calculate AUC and prepare for ROC curve for gradient boosting
gradient_prediction_2 = predict(gforest_2, test_2,
                                n.trees=gforest_2$n.trees,type = "response")
gradient_rocdata_2 = prediction(predictions=gradient_prediction_2,
                                labels=test_2$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_gradient_2 = prediction(predictions=gradient_prediction_2,
                            labels=test_2$Y) %>% performance("auc")
auc_gradient_2 = round(auc_gradient_2@y.values[[1]],3)


# Calculate AUC and prepare for ROC curve for logistic regularized regression
lasso_prediction_2 = predict(lasso_2, mtest_2, s= bestlam_2, type='response')
lasso_rocdata_2 = prediction(predictions=lasso_prediction_2,
                             labels=test_2$Y) %>%
  performance("tpr", "fpr") %>%
  (function(.) data.frame(FPR=.@x.values[[1]], TPR=.@y.values[[1]]) %>% as_tibble())(.) 

auc_lasso_2 =  prediction(predictions=lasso_prediction_2,
                          labels=test_2$Y) %>% performance("auc")
auc_lasso_2 = round(auc_lasso_2@y.values[[1]],3)


# Plot ROC curve
ggplot(data = logit_rocdata_2, aes(x=FPR,y=TPR, col = paste0("Logistic Regression: AUC", auc_lg_2))) + geom_line() +
  geom_line(data = rf_rocdata_2, aes(x=FPR,y=TPR, col = paste0("RF: AUC", auc_rf_2))) +
  geom_line(data = boosting_rocdata_2, aes(x=FPR,y=TPR, col = paste0("Boosting: AUC", auc_boosting_2))) +
  geom_line(data = gradient_rocdata_2, aes(x=FPR,y=TPR, col = paste0("Gradient boosting: AUC", auc_gradient_2))) + 
  geom_line(data = lasso_rocdata_2, aes(x=FPR,y=TPR, col = paste0("Lasso: AUC", auc_lasso_2))) +
  theme(legend.title=element_blank())

```
```{r}
## Appendix C.
# Since we could not see considerable improvement of our model even when adding 2 more features, we explore a neural network model with dataset we created with new features.

# First, create input variables data frame and output variable data frame to create dummy variables.
xtrain_2 = train_2 %>% select(-Y) 
xtest_2 = test_2 %>% select(-Y) 

# Also, modify the outcome (Y) into numeric 
ytrain_2 = ordered(train_2$Y,
                   levels = c("N","Y"),
                   labels = c(0, 1)) %>% as.matrix()

ytest_2 = ordered(test_2$Y,
                 levels = c("N","Y"),
                 labels = c(0, 1)) %>% as.matrix()
  
# Then, create dummy variables for input variables.
dmy_train_2 = dummyVars(~., data = xtrain_2)
trsf_train_2 <- as.data.frame(predict(dmy_train_2,xtrain_2)) %>% as.matrix()

dmy_test_2 = dummyVars(~ ., data = xtest_2)
trsf_test_2 <- as.data.frame(predict(dmy_test_2,xtest_2)) %>% as.matrix()

# Report demension of the training set
dim(trsf_train_2)

# Specify a model architecture
model = keras_model_sequential() 

# Add layers
# Based on the previous sections, adding regularization shows the similar performance as the one without regularization.
# Therefore, we do not use regularization in our network.
# As an activation function for hidden layers, we use ReLU, which is the most common one.
# Since our purpose is classification, we use sigmoid output unit.
model %>%  
  layer_dense(units = 32, 
              activation = 'relu',
              input_shape = c(ncol(trsf_train_2))) %>%  
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'relu') %>%  
  layer_dense(units = 1, activation = 'sigmoid')  

summary(model)

# Specify loss and optimization method
# We use mean squared error to optimize the model
model %>% compile(
  loss = c('mse'),
  optimizer = optimizer_nadam(clipnorm = 10),
  metrics = c('mse')
)

# Train model with training dataset
inner_epochs = 10
early_stopping = callback_early_stopping(monitor = "val_loss",
                                         patience = inner_epochs/2)
bestLoss = 1e10
for(i in 1:20) {
  history = model %>% fit(trsf_train_2 , ytrain_2,
                          epochs = inner_epochs,
                          callbacks = c(early_stopping),
                          batch_size = 16,  
                          validation_split = 0.2, shuffle=T)
  loss = history$metrics$val_loss[length(history$metrics$val_loss)]
  if(loss < bestLoss) {
    bestLoss = loss
    model %>% save_model_weights_hdf5("my_model_weights.h5")
  }
  if(length(history$metrics$val_loss) < inner_epochs)
    break
}

### Plot performance 
plot(history, metrics = "loss")  # only plots the last part of training

### Load the early-stopping model
bestModel = model %>% load_model_weights_hdf5('my_model_weights.h5')
bestModel %>% compile(
  loss = 'mse',
  optimizer = optimizer_nadam(),
  metrics = c('mse')
)

### Make predictions
bestModel %>% evaluate(trsf_test_2, ytest_2)
predition_2 = bestModel %>% predict_on_batch(trsf_test_2) 

# Show predictions and true classification
rownum = 1:nrow(ytest_2)
ytest = cbind(ytest_2,rownum)
summarytable = cbind(ytest_2, predition_2[,1]) %>% as.data.frame()
summarytable

```

